{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final IR-Project(Team No. 11).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zKP5JaGhtC9A",
        "zjHbJtNsCND2",
        "TuvR5Gx3HDy_",
        "xE2WsAd8YF-J",
        "5bkOGAMyRm5I",
        "5VHzp_NpCVxb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKP5JaGhtC9A"
      },
      "source": [
        "# Extracting from XML file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmJNgDhEtLaj",
        "outputId": "8bdaeda1-6ef9-41ac-97cc-bf5e8e4c5677"
      },
      "source": [
        "!git clone https://github.com/arpi-r/IR-Project.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IR-Project'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 72407 (delta 0), reused 3 (delta 0), pack-reused 72403\u001b[K\n",
            "Receiving objects: 100% (72407/72407), 115.50 MiB | 19.70 MiB/s, done.\n",
            "Resolving deltas: 100% (39829/39829), done.\n",
            "Checking out files: 100% (48088/48088), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix7nUQ32tNfR",
        "outputId": "37b2a6c3-be42-420a-a529-eda64c1ae8e2"
      },
      "source": [
        "pip install Unidecode"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 22.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 27.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 20.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 14.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 13.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 12.5MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0X228c3tQPh",
        "outputId": "57c787d7-d3f3-4b74-846a-c0f3c5a2698d"
      },
      "source": [
        "import os\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "import nltk\n",
        "import unidecode\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "from prettytable import PrettyTable\n",
        "from nltk.tokenize import word_tokenize\n",
        "import xml.etree.cElementTree as ET\n",
        "import csv\n",
        "import json\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSZmo-RkbRWs"
      },
      "source": [
        "def remove_extra_space(text):\n",
        "  words = text.strip().split()\n",
        "  text = \" \".join(words)\n",
        "  return text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3irFEypvtXOM"
      },
      "source": [
        "def parse_xml_file(filepath):\n",
        "  tree = ET.parse(filepath)\n",
        "  root = tree.getroot()\n",
        "  trial_info = {}\n",
        "  \n",
        "  for child in root:\n",
        "    if child.tag == 'brief_title' or child.tag == 'keyword':\n",
        "      trial_info[child.tag] = child.text\n",
        "    \n",
        "    if child.tag == 'condition':\n",
        "      if not child.tag in trial_info:\n",
        "        trial_info[child.tag] = []\n",
        "      trial_info[child.tag].append(child.text)\n",
        "    \n",
        "    if child.tag == 'intervention':\n",
        "      for ch in child:\n",
        "        if not ch.tag in trial_info:\n",
        "          trial_info[ch.tag] = []\n",
        "        trial_info[ch.tag].append(ch.text)\n",
        "    \n",
        "    if child.tag == 'brief_summary' or child.tag == 'detailed_description':\n",
        "      trial_info[child.tag] = remove_extra_space(child.getchildren()[0].text)\n",
        "    \n",
        "    if child.tag == 'eligibility':\n",
        "      for ch in child:\n",
        "        if ch.tag == 'criteria':\n",
        "          trial_info[ch.tag] = remove_extra_space(ch.getchildren()[0].text)\n",
        "        else:\n",
        "          trial_info[ch.tag] = ch.text      \n",
        "    \n",
        "    if child.tag == 'condition_browse' or child.tag == 'intervention_browse':\n",
        "      for ch in child:\n",
        "        if ch.tag == 'mesh_term':\n",
        "          if not ch.tag in trial_info:\n",
        "            trial_info[ch.tag] = []\n",
        "          trial_info[ch.tag].append(ch.text)\n",
        "\n",
        "  return trial_info"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDg5TZUucmSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924380a6-bc66-4de1-e9dd-cb461ade15c0"
      },
      "source": [
        "ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mIR-Project\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjHbJtNsCND2"
      },
      "source": [
        "# Basic Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRVReK1rBJrz"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk.stem as st\n",
        "from nltk import WordNetLemmatizer\n",
        "import unidecode\n",
        "import os"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR7bZnyDBQOO"
      },
      "source": [
        "def preprocessData(text):\n",
        "    # tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # remove punctualtion and retain only alpha-numeric tokens\n",
        "    tokens = [word for word in tokens if word.isalpha() or word.isdigit()]\n",
        "\n",
        "    # normalize the tokens\n",
        "    # lowercase\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    # remove accents\n",
        "    tokens = [unidecode.unidecode(word) for word in tokens]\n",
        "    # remove stopwords\n",
        "    tokens = [word for word in tokens if not word in stopwords.words('english')]\n",
        "\n",
        "    # lemmatize\n",
        "    lemma = WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(word, pos = 'v') for word in tokens]\n",
        "    tokens = [lemma.lemmatize(word, pos = 'n') for word in tokens]\n",
        "    return tokens\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA4A5HJCBWfx"
      },
      "source": [
        "# vocabulary = set()\n",
        "# for doc in os.listdir('Json_Data'):\n",
        "#     with open(os.path.join('Json_Data', doc)) as json_file: \n",
        "#         try:\n",
        "#             data = json.load(json_file)\n",
        "#         except json.decoder.JSONDecodeError:\n",
        "#             continue\n",
        "#         for key in data:\n",
        "#             if key == \"id\":\n",
        "#                 continue\n",
        "#             elif key == \"gender\" or key == \"healthy_volunteers\" or key == \"intervention_type\" or key == \"intervention_name\":\n",
        "#                 vocabulary.update(data[key])\n",
        "#                 continue\n",
        "#             else:\n",
        "#                 data[key] = preprocessData(str(data[key]))\n",
        "#                 vocabulary.update(data[key])\n",
        "#         out_file = open('./Preprocessed-Json/' + doc, \"w\") \n",
        "#         json.dump(data, out_file, indent = 6) \n",
        "#         out_file.close()\n",
        "# with open('vocabulary.txt','w') as f:\n",
        "#     f.write(str(vocabulary))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h0HF4L0C9sr"
      },
      "source": [
        "# !zip -r /content/Preprocessed-Json.zip /content/Preprocessed-Json"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_sD3wrCC9st"
      },
      "source": [
        "# from google.colab import files\n",
        "# files.download(\"/content/Preprocessed-Json.zip\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcRD6kf0DNBW"
      },
      "source": [
        "# files.download(\"/content/vocabulary.txt\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuvR5Gx3HDy_"
      },
      "source": [
        "# Access to preprocessed data\n",
        "\n",
        "The above 2 steps have been run and the data is uploaded on GitHub. Their result can directly be accessed from this section. Only run cells containing imports and the preprocessData() method in the previous 2 sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh-AaB1rM7DI"
      },
      "source": [
        "# rm -rf ./IR-Project"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAYIO83IHUWO"
      },
      "source": [
        "# !git clone https://github.com/arpi-r/IR-Project.git"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE2WsAd8YF-J"
      },
      "source": [
        "# Extract Test Queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKKaokbBTxGq"
      },
      "source": [
        "def parse_query_xml_file():\n",
        "  filepath = '/content/IR-Project/queries/queries.xml'\n",
        "  tree = ET.parse(filepath)\n",
        "  root = tree.getroot()\n",
        "  queries = []\n",
        "  count = 1\n",
        "  for child in root:\n",
        "    query = {}\n",
        "    unstructured_data = \"\"\n",
        "    age = -1\n",
        "    gender = \"\"\n",
        "    other = \"\"\n",
        "    mesh_terms = []\n",
        "\n",
        "    for ch in child:\n",
        "      if ch.tag == 'other':\n",
        "        if not ch.text == 'None':\n",
        "          unstructured_data += \" \" + ch.text\n",
        "          other = ch.text\n",
        "\n",
        "      if ch.tag == 'disease':\n",
        "        unstructured_data += ch.text\n",
        "        query[ch.tag] = ch.text\n",
        "\n",
        "      if ch.tag == 'gene':\n",
        "        unstructured_data += \" \" + ch.text\n",
        "        query[ch.tag] = ch.text\n",
        "\n",
        "      if ch.tag == 'demographic':\n",
        "        info = ch.text.split(' ')\n",
        "        age = int(info[0].split('-')[0])\n",
        "        gender = info[1]\n",
        "      \n",
        "      if ch.tag == 'mesh_term':\n",
        "        mesh_terms.append(ch.text)\n",
        "    query['qid'] = count\n",
        "    query['unstructured_data'] = unstructured_data\n",
        "    query['age'] = age\n",
        "    query['gender'] = gender\n",
        "    query['other'] = other\n",
        "    query['mesh_terms'] = mesh_terms\n",
        "    count = count + 1\n",
        "\n",
        "    queries.append(query)\n",
        "\n",
        "  return queries"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWNrgpzMUnZl"
      },
      "source": [
        "import random\n",
        "def generateFold(queries, percent):\n",
        "  train = []\n",
        "  test = []\n",
        "  visited = [False for i in range(len(queries))]\n",
        "  while len(train) <= percent *  len(queries):\n",
        "      ind = random.randint(0, len(queries) - 1)\n",
        "      if not visited[ind]:\n",
        "        train.append(queries[ind])\n",
        "        visited[ind] = True\n",
        "  \n",
        "  for i in range(len(queries)):\n",
        "    if not visited[i]:\n",
        "      test.append(queries[i])\n",
        "  return train, test\n",
        "def generate_K_Folds(queries, percent, K):\n",
        "  K_folds_Train = list()\n",
        "  K_folds_Test = list()\n",
        "  for i in range(K):\n",
        "    train, test = generateFold(queries, percent)\n",
        "    K_folds_Train.append(train)\n",
        "    K_folds_Test.append(test)\n",
        "  return K_folds_Train, K_folds_Test"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bkOGAMyRm5I"
      },
      "source": [
        "# Exact match with structured data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C5JoYbORtXe"
      },
      "source": [
        "def extract_structured_data_for_exact_match():\n",
        "  path = '/content/IR-Project/Preprocessed-Json/'\n",
        "\n",
        "  docs = {}\n",
        "  for root, directories, files in os.walk(path, topdown=False):\n",
        "    for name in files:\n",
        "      filepath = (os.path.join(root,name))\n",
        "      fileobj = open(filepath, 'r')\n",
        "      file_data = json.load(fileobj)\n",
        "      fileobj.close()\n",
        "      if \"xml\" in file_data['id']:\n",
        "        continue\n",
        "      structured_data = []\n",
        "      \n",
        "      if 'minimum_age' in file_data:\n",
        "        structured_data.append(file_data['minimum_age'])\n",
        "      else:\n",
        "        structured_data.append(0)\n",
        "\n",
        "      if 'maximum_age' in file_data:\n",
        "        structured_data.append(file_data['maximum_age'])\n",
        "      else:\n",
        "        structured_data.append(150)\n",
        "\n",
        "      if 'gender' in file_data:\n",
        "        structured_data.append(file_data['gender'])\n",
        "      else:\n",
        "        structured_data.append('all')\n",
        "\n",
        "      if 'mesh_term' in file_data:\n",
        "        structured_data.append(file_data['mesh_term'])\n",
        "      else:\n",
        "        structured_data.append([])\n",
        "      \n",
        "      docs[file_data['id']] = structured_data\n",
        "\n",
        "\n",
        "  # print(len(docs))\n",
        "  # print(docs['NCT00000108'])\n",
        "\n",
        "  return docs"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6rY98IUicvA"
      },
      "source": [
        "def get_relevant_set(query, docs):\n",
        "  qage = query['age']\n",
        "  qgen = query['gender']\n",
        "  qmesh = query['mesh_terms']\n",
        "\n",
        "  data_subset = set()\n",
        "  l = len(docs)\n",
        "\n",
        "  for key in docs.keys():\n",
        "    doc = docs[key]\n",
        "    # print(doc)\n",
        "    age = True\n",
        "    gender = True\n",
        "    mesh_match = True\n",
        "\n",
        "    if type(doc[0]) == int:\n",
        "      if qage > doc[0] or qage < doc[1]:\n",
        "        age = False\n",
        "    else:\n",
        "      if len(doc[0]) < 2 and len(doc[1]) < 2:\n",
        "        age = True\n",
        "      elif len(doc[0]) < 2:\n",
        "        if not (doc[1][1] == 'year'):\n",
        "          age = False\n",
        "        if qage > int(doc[1][0]):\n",
        "          age = False\n",
        "      elif len(doc[1]) < 2:\n",
        "        if not (doc[0][1] == 'year'):\n",
        "          age = False\n",
        "        if qage < int(doc[0][0]):\n",
        "          age = False\n",
        "      else:\n",
        "        if qage > int(doc[1][0]) or qage < int(doc[0][0]):\n",
        "          age = False\n",
        "\n",
        "      if not (doc[2].lower() == 'all'):\n",
        "        if not (qgen.lower() == doc[2].lower()):\n",
        "          gender = False\n",
        "      \n",
        "      mesh = ' '.join(qmesh)\n",
        "      mesh_terms = preprocessData(mesh)\n",
        "\n",
        "      common_count = len(set(mesh_terms)&set(doc[3]))\n",
        "\n",
        "      if common_count < 1:\n",
        "        mesh_match = False\n",
        "      \n",
        "      if age and gender and mesh_match:\n",
        "        data_subset.add(key)\n",
        "  \n",
        "  return data_subset"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAtL2eqzHuWs"
      },
      "source": [
        "# docs = extract_structured_data_for_exact_match()\n",
        "# query = queries[1]\n",
        "# data_subset = get_relevant_set(query, docs)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VHzp_NpCVxb"
      },
      "source": [
        "# BM25 Model to extract relevant documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ1moFyeLE-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0eea08-31d6-4c28-de53-16308034faac"
      },
      "source": [
        "!pip install rank_bm25"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.18.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT_XkiXkLG5e"
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import os\n",
        "import json\n",
        "import heapq "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDR6DqW8CeaA"
      },
      "source": [
        "def extract_unstructured_data_for_relevance_classification(data_subset):\n",
        "  path = '/content/IR-Project/Preprocessed-Json/'\n",
        "\n",
        "  corpus_data = []\n",
        "  doc_id = []\n",
        "  for root, directories, files in os.walk(path, topdown=False):\n",
        "    for name in files:\n",
        "      filepath = (os.path.join(root,name))\n",
        "      fileobj = open(filepath, 'r')\n",
        "      file_data = json.load(fileobj)\n",
        "      fileobj.close()\n",
        "      if \"xml\" in file_data['id'] or file_data['id'] not in data_subset:\n",
        "        continue\n",
        "      # print(file_data['id'])\n",
        "      doc_id.append(file_data['id'])\n",
        "      # summary = []\n",
        "      unstructured_data = []\n",
        "      if 'brief_title' in file_data:\n",
        "        unstructured_data += file_data['brief_title']\n",
        "      if 'brief_summary' in file_data:\n",
        "        unstructured_data += file_data['brief_summary']\n",
        "      # description = []\n",
        "      if 'detailed_description' in file_data:\n",
        "        unstructured_data += file_data['detailed_description']\n",
        "      # unstructured_data = summary + description\n",
        "      if 'criteria' in file_data:\n",
        "        unstructured_data += file_data['criteria']\n",
        "      if 'mesh_term' in file_data:\n",
        "        unstructured_data += file_data['mesh_term']\n",
        "      if 'keyword' in file_data:\n",
        "        unstructured_data += file_data['keyword']\n",
        "      # corpus_data.append(summary + description)\n",
        "      corpus_data.append(unstructured_data)\n",
        "\n",
        "  # print(len(corpus_data))\n",
        "  # print(corpus_data[0])\n",
        "  # print(len(corpus_data[0]))\n",
        "\n",
        "  return doc_id, corpus_data"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t6x8k8pJQ-m"
      },
      "source": [
        "def get_bm25_model(data):\n",
        "  bm25 = BM25Okapi(data)\n",
        "  return bm25"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yvVuZuxJLhA"
      },
      "source": [
        "def preprocess_query_bm25(query):\n",
        "  tokenized_query = preprocessData(query)\n",
        "  return tokenized_query"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lozg4H0fJUoV"
      },
      "source": [
        "def find_relevancy(query, bm25, doc_id):\n",
        "  preprocessed_query = preprocess_query_bm25(query)\n",
        "  doc_scores = bm25.get_scores(preprocessed_query)\n",
        "\n",
        "  # print(\"Doc scores: \", doc_scores)\n",
        "\n",
        "  # ranked_heap = []\n",
        "  # heapq.heapify(ranked_heap)\n",
        "\n",
        "  l = len(doc_id)\n",
        "  # for i in range(l):\n",
        "  #   heapq.heappush(ranked_heap, (-doc_scores[i], doc_id[i]))\n",
        "\n",
        "  relevant_docs = []\n",
        "  irrelevant_docs = []\n",
        "\n",
        "  irrelevant_count = 0\n",
        "  for i in range(l):\n",
        "    if doc_scores[i] == 0.0:\n",
        "      irrelevant_count += 1\n",
        "      irrelevant_docs.append(doc_id[i])\n",
        "    else:\n",
        "      relevant_docs.append(doc_id[i])\n",
        "  relevant_count = l - irrelevant_count\n",
        "\n",
        "  # print(\"Number of relevant docs: \", relevant_count)\n",
        "  # print(\"Number of irrelevant docs: \", irrelevant_count)\n",
        "\n",
        "  # doc_relevance_grade = []\n",
        "  # count_per_grade = relevant_count // 3\n",
        "  # count_so_far = 0\n",
        "  # grade = 3\n",
        "\n",
        "  # for i in range(0, relevant_count):\n",
        "  #   doc = heapq.heappop(ranked_heap)[1]\n",
        "  #   if (count_so_far > count_per_grade):\n",
        "  #     count_so_far = 0\n",
        "  #     grade -= 1\n",
        "  #   count_so_far += 1\n",
        "  #   doc_relevance_grade.append((doc, grade))\n",
        "\n",
        "  # for i in range(0, irrelevant_count):\n",
        "  #   doc = heapq.heappop(ranked_heap)[1]\n",
        "  #   doc_relevance_grade.append((doc, 0))\n",
        "\n",
        "  # print(\"Documents relevancy grade: \")\n",
        "  # print(doc_relevance_grade)\n",
        "\n",
        "  # for i in range(l):\n",
        "  #   if doc_relevance_grade[i][1] == 0:\n",
        "  #     irrelevant_docs.append(doc_relevance_grade[i][0])\n",
        "  #   else:\n",
        "  #     relevant_docs.append(doc_relevance_grade[i][0])\n",
        "  \n",
        "  # print(\"Relevant docs: \", relevant_docs)\n",
        "  # print(\"Irrelevant docs: \", irrelevant_docs)\n",
        "\n",
        "  # return doc_relevance_grade, relevant_docs, irrelevant_docs\n",
        "  return relevant_docs, irrelevant_docs"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhqIG-QfJaHp"
      },
      "source": [
        "# query = queries[1]['unstructured_data']\n",
        "# doc_id, data_corpus = extract_unstructured_data_for_relevance_classification(data_subset)\n",
        "# bm25 = get_bm25_model(data_corpus)\n",
        "# print(query, bm25, doc_id)\n",
        "# relevant_docs, irrelevant_docs = find_relevancy(query, bm25, doc_id)\n",
        "\n",
        "# print(relevant_docs)\n",
        "# print(irrelevant_docs)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOHCVIffQpyo"
      },
      "source": [
        "# Query Reformulation - Rocchio's Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRwtMSHjVLHA"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import csc_matrix\n",
        "from numpy import asarray\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqWunXPKPVnF"
      },
      "source": [
        "def extract_vsm_for_query_reformulation(query, data_subset):\n",
        "  path = '/content/IR-Project/Preprocessed-Json/'\n",
        "  corpus_data = []\n",
        "  doc_ids = []\n",
        "\n",
        "  for root, directories, files in os.walk(path, topdown=False):\n",
        "    for name in files:\n",
        "      filepath = (os.path.join(root,name))\n",
        "      fileobj = open(filepath, 'r')\n",
        "      file_data = json.load(fileobj)\n",
        "      fileobj.close()\n",
        "      if \"xml\" in file_data['id'] or file_data['id'] not in data_subset:\n",
        "        continue\n",
        "      # print(file_data['id'])\n",
        "      doc_ids.append(file_data['id'])\n",
        "      summary = []\n",
        "      if 'brief_summary' in file_data:\n",
        "        summary = file_data['brief_summary']\n",
        "      description = []\n",
        "      if 'detailed_description' in file_data:\n",
        "        description = file_data['detailed_description']\n",
        "      sum_des = summary + description\n",
        "      s = ' '.join(sum_des)\n",
        "      corpus_data.append(s)\n",
        "\n",
        "  # print(corpus_data[0])\n",
        "  corpus_data.append(query)\n",
        "  \n",
        "  vectorizer = TfidfVectorizer()\n",
        "  tf = TfidfVectorizer(input=corpus_data, analyzer='word',\n",
        "                      min_df = 0, stop_words = 'english', sublinear_tf=True)\n",
        "  tfidf_matrix =  tf.fit_transform(corpus_data).toarray()\n",
        "\n",
        "  data_vsm = tfidf_matrix[:-1][:]\n",
        "  query_vsm = tfidf_matrix[-1][:]\n",
        "\n",
        "  return tf, data_vsm, query_vsm, doc_ids"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuxJI_xnK8ZT"
      },
      "source": [
        "def preprocess_query_for_reformulation(query):\n",
        "  preprocessed_query = preprocessData(query)\n",
        "  preprocessed_query = ' '.join(preprocessed_query)\n",
        "  # print(preprocessed_query)\n",
        "  return preprocessed_query"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJGszuXhObyI"
      },
      "source": [
        "def rocchios_query_reformulation(query, relevant, irrelevant, beta, gamma, docs, doc_ids):\n",
        "  updated_query = query[:]\n",
        "  l = len(doc_ids)\n",
        "  rl = len(relevant)\n",
        "  irl = len(irrelevant)\n",
        "  if not (rl == 0):\n",
        "    c1 = (beta / rl)\n",
        "  else:\n",
        "    c1 = 0\n",
        "  if not (irl == 0):\n",
        "    c2 = (gamma / irl)\n",
        "  else:\n",
        "    c2 = 0\n",
        "\n",
        "  for i in range(l):\n",
        "    if doc_ids[i] in relevant:\n",
        "      updated_query += (c1 * docs[i])\n",
        "    else:\n",
        "      updated_query -= (c2 * docs[i])\n",
        "    \n",
        "    updated_query = updated_query.clip(0)\n",
        "\n",
        "  return updated_query"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6yDm-Z0GRAF"
      },
      "source": [
        "def calc_similarity(docs_vsm, query_vsm):\n",
        "  l = docs_vsm.shape[0]\n",
        "  similarities = []\n",
        "\n",
        "  for i in range(l):\n",
        "    cos_sim = dot(docs_vsm[i], query_vsm)/(norm(docs_vsm[i])*norm(query_vsm))\n",
        "    similarities.append(cos_sim)\n",
        "\n",
        "  return similarities"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kICaIPKvPVH_"
      },
      "source": [
        "def rank_cosine(similarities, doc_ids):\n",
        "    ranked_heap = []\n",
        "    heapq.heapify(ranked_heap)\n",
        "    l = len(doc_ids)\n",
        "\n",
        "    if not(len(similarities) == l):\n",
        "      print(\"someting's wrong!\")\n",
        "      return ranked_heap\n",
        "\n",
        "    for i in range(l):\n",
        "      heapq.heappush(ranked_heap, (-similarities[i], doc_ids[i]))\n",
        "    \n",
        "    ranked_list = []\n",
        "\n",
        "    for i in range(l):\n",
        "      ranked_list.append(heapq.heappop(ranked_heap)[1])\n",
        "    \n",
        "    return ranked_list"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeMhQv5VPK3a"
      },
      "source": [
        "# query = queries[1]['unstructured_data']\n",
        "# preprocessed_query = preprocess_query_for_reformulation(query)\n",
        "# tfidfvectorizer, docs_vsm, query_vsm, doc_ids = extract_vsm_for_query_reformulation(preprocessed_query, data_subset)\n",
        "# print(docs_vsm.shape)\n",
        "# print(query_vsm.shape)\n",
        "# beta = 0.75\n",
        "# gamma = 0.25\n",
        "# runs = 10\n",
        "# reformulated_query = query_vsm\n",
        "\n",
        "# for i in range(runs):\n",
        "#   reformulated_query = rocchios_query_reformulation(reformulated_query, relevant_docs, irrelevant_docs, beta, gamma, docs_vsm, doc_ids)\n",
        "\n",
        "# # print(\"Original Query: \", query_vsm)\n",
        "# # print(\"Reformulated Query: \", reformulated_query)\n",
        "# print(sum(query_vsm))\n",
        "# print(sum(reformulated_query))\n",
        "\n",
        "# similarities = calc_similarity(docs_vsm, reformulated_query)\n",
        "# print(\"\\nSimilarities: \", similarities)\n",
        "\n",
        "# ranked_list = rank_cosine(similarities, doc_ids)\n",
        "# print(\"\\nRanking: \", ranked_list)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCWOFJ1nZPY_"
      },
      "source": [
        "# def query_reformulator(query, data_subset, beta, gamma, runs, relevant_docs, irrelevant_docs):\n",
        "#   query = query['unstructured_data']\n",
        "#   preprocessed_query = preprocess_query_for_reformulation(query)\n",
        "#   tfidfvectorizer, docs_vsm, query_vsm, doc_ids = extract_vsm_for_query_reformulation(preprocessed_query, data_subset)\n",
        "#   reformulated_query = query_vsm\n",
        "\n",
        "#   for i in range(runs):\n",
        "#     reformulated_query = rocchios_query_reformulation(reformulated_query, relevant_docs, irrelevant_docs, beta, gamma, docs_vsm, doc_ids)\n",
        "\n",
        "#   similarities = calc_similarity(docs_vsm, reformulated_query)\n",
        "#   ranked_list = rank_cosine(similarities, doc_ids)\n",
        "\n",
        "#   return ranked_list\n",
        "\n",
        "\n",
        "# def train_model(train_queries, docs, rocchio_iteration, beta, gamma) :\n",
        "#   ranked_list = {}\n",
        "#   for query in train_queries:\n",
        "#     print(query)\n",
        "#     data_subset = get_relevant_set(query, docs)\n",
        "#     doc_id, data_corpus = extract_unstructured_data_for_relevance_classification(data_subset)\n",
        "#     bm25 = get_bm25_model(data_corpus)\n",
        "#     relevant_docs, irrelevant_docs = find_relevancy(query['unstructured_data'], bm25, doc_id)\n",
        "#     ranked_list_after_rocchio = query_reformulator(query, data_subset, beta, gamma, rocchio_iteration, relevant_docs, irrelevant_docs)\n",
        "#     ranked_list[query['qid']] = ranked_list_after_rocchio\n",
        "#   train_LambdaRank(ranked_list)\n",
        "\n",
        "# def main():\n",
        "#   docs =  extract_structured_data_for_exact_match()\n",
        "#   queries = parse_query_xml_file()\n",
        "#   percent = 0.8\n",
        "#   K = 10\n",
        "#   r_itr = 10\n",
        "#   beta = 0.75\n",
        "#   gamma =  0.25\n",
        "#   train, test = generate_K_Folds(queries, percent, K)\n",
        "#   for i in range(len(train)):\n",
        "#     train_model(train[i], docs, r_itr, beta, gamma)\n",
        "#     break\n",
        "\n",
        "# main()\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqtOudNPyQHC"
      },
      "source": [
        "# LamdaRank\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDoRRGoIT24n"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def get_corpus():\n",
        "  path = '/content/IR-Project/Preprocessed-Json/'\n",
        "  corpus_data = {}\n",
        "  doc_id = []\n",
        "\n",
        "  for root, directories, files in os.walk(path, topdown=False):\n",
        "    for name in files:\n",
        "      filepath = (os.path.join(root,name))\n",
        "      fileobj = open(filepath, 'r')\n",
        "      file_data = json.load(fileobj)\n",
        "      fileobj.close()\n",
        "      if \"xml\" in file_data['id']:\n",
        "        continue\n",
        "      # print(file_data['id'])\n",
        "      doc_id.append(file_data['id'])\n",
        "      summary = []\n",
        "      if 'brief_summary' in file_data:\n",
        "        summary = file_data['brief_summary']\n",
        "      description = []\n",
        "      if 'detailed_description' in file_data:\n",
        "        description = file_data['detailed_description']\n",
        "      sum_des = summary + description\n",
        "      s = ' '.join(sum_des)\n",
        "      corpus_data[name[:len(name) - 5]] = s\n",
        "  return corpus_data\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvmqMZ8VuNrz",
        "outputId": "f250da25-87b2-4dd9-d676-365c38b728fb"
      },
      "source": [
        "!pip install LambdaRankNN\n",
        "!pip install gensim"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting LambdaRankNN\n",
            "  Downloading https://files.pythonhosted.org/packages/2d/96/b10f198c7d652c50c4e1458a8517e49885aa79e1d288e96e1d0a9c74092b/LambdaRankNN-0.1.1-py3-none-any.whl\n",
            "Installing collected packages: LambdaRankNN\n",
            "Successfully installed LambdaRankNN-0.1.1\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC-qQ57ruRIX"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "def getDoc2Vec(corpus_data):\n",
        "  documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_data)]\n",
        "  model = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4)\n",
        "  w2v = {}\n",
        "  for doc in corpus_data:\n",
        "    w2v[doc] = model.infer_vector(corpus_data[doc])\n",
        "  return model, w2v"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6uk5edIuUdn"
      },
      "source": [
        "import pandas as pd\n",
        "def getRelevanceJudgement():\n",
        "  relevancePath = '/content/IR-Project/queries/relevance.txt'\n",
        "\n",
        "  data = pd.read_csv(relevancePath, header = None, delimiter = \" \")\n",
        "  relevance = {}\n",
        "  qid = data[0]\n",
        "  docid = data[2]\n",
        "  relevanceJudgement = data[3]\n",
        "  for i in range(len(docid)):\n",
        "      relevance[(qid[i],docid[i])] = relevanceJudgement[i]\n",
        "  return relevance\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8eE5DgIuXOx"
      },
      "source": [
        "def getLambdaRankTrainData(temp_rankings, w2v, relevance):\n",
        "  LambdaRank_X = []\n",
        "  LambdaRank_Y = []\n",
        "  LambdaRank_qid = []\n",
        "  LambdaRank_docid = []\n",
        "  for i in range(20):\n",
        "    query = i + 1\n",
        "    part_count = 10 \n",
        "    temp_count = 0\n",
        "    relgrade = 2\n",
        "    for doc in temp_rankings[query]:\n",
        "      if temp_count == part_count and not (relgrade == 0):\n",
        "        relgrade -= 1\n",
        "        temp_count = 0\n",
        "        part_count = 20\n",
        "      temp_count += 1\n",
        "      LambdaRank_X.append(w2v[doc])\n",
        "      LambdaRank_qid.append(query)\n",
        "      LambdaRank_docid.append(doc)\n",
        "      if (query, doc) in relevance:\n",
        "        LambdaRank_Y.append(relevance[(query, doc)])\n",
        "      else:\n",
        "        LambdaRank_Y.append(relgrade)\n",
        "  return  LambdaRank_X, LambdaRank_Y, LambdaRank_qid, LambdaRank_docid\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_ccjsYv_P9p"
      },
      "source": [
        "def getLambdaRankTestData(temp_rankings, w2v, relevance):\n",
        "  LambdaRank_X_test = []\n",
        "  LambdaRank_Y_test = []\n",
        "  LambdaRank_qid_test = []\n",
        "  for i in range(20, 30):\n",
        "    query = i + 1\n",
        "    part_count = 10 \n",
        "    temp_count = 0\n",
        "    relgrade = 2\n",
        "    for doc in temp_rankings[query]:\n",
        "      if temp_count == part_count and not (relgrade == 0):\n",
        "        relgrade -= 1\n",
        "        temp_count = 0\n",
        "        part_count = 20\n",
        "      temp_count += 1\n",
        "      LambdaRank_X_test.append(w2v[doc])\n",
        "      LambdaRank_qid_test.append(query)\n",
        "      if (query, doc) in relevance:\n",
        "        LambdaRank_Y_test.append(relevance[(query, doc)])\n",
        "      else:\n",
        "        LambdaRank_Y_test.append(relgrade)\n",
        "  return  LambdaRank_X_test, LambdaRank_Y_test, LambdaRank_qid_test"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8j3CRjvud63"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from LambdaRankNN import LambdaRankNN\n",
        "\n",
        "# generate query data\n",
        "def fitLambda(LambdaRank_X, LambdaRank_Y, LambdaRank_qid):\n",
        "  X = np.array(LambdaRank_X)\n",
        "  y = np.array(LambdaRank_Y)\n",
        "  qid = np.array(LambdaRank_qid)\n",
        "  ranker = LambdaRankNN(input_size=X.shape[1], hidden_layer_sizes=(16,8,), activation=('relu', 'relu',), solver='adam')\n",
        "  ranker.fit(X, y, qid, epochs=5, batch_size = 64)\n",
        "  return ranker"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cho1-BiNcsy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2c41ba-7cdd-4790-f2c5-2e045b96efac"
      },
      "source": [
        "def train_LambdaRank(ranked_list, corpus_data): \n",
        "  d3v_model, w2v = getDoc2Vec(corpus_data)\n",
        "  relevance = getRelevanceJudgement()\n",
        "  x, y, qid, docid = getLambdaRankTrainData(ranked_list, w2v, relevance)\n",
        "  x_test, y_test, qid_test = getLambdaRankTestData(ranked_list, w2v, relevance)\n",
        "  ranker = fitLambda(x, y, qid)\n",
        "  ranker.evaluate(np.array(x_test), np.array(y_test), np.array(qid_test), eval_at = 2)\n",
        "\n",
        "\n",
        "def query_reformulator(query, data_subset, beta, gamma, runs, relevant_docs, irrelevant_docs):\n",
        "  query = query['unstructured_data']\n",
        "  preprocessed_query = preprocess_query_for_reformulation(query)\n",
        "  tfidfvectorizer, docs_vsm, query_vsm, doc_ids = extract_vsm_for_query_reformulation(preprocessed_query, data_subset)\n",
        "  reformulated_query = query_vsm\n",
        "\n",
        "  for i in range(runs):\n",
        "    reformulated_query = rocchios_query_reformulation(reformulated_query, relevant_docs, irrelevant_docs, beta, gamma, docs_vsm, doc_ids)\n",
        "\n",
        "  similarities = calc_similarity(docs_vsm, reformulated_query)\n",
        "  ranked_list = rank_cosine(similarities, doc_ids)\n",
        "\n",
        "  return ranked_list\n",
        "\n",
        "\n",
        "def train_model(train_queries, docs, rocchio_iteration, beta, gamma, fold, t) :\n",
        "  ranked_list = {}\n",
        "  for query in train_queries:\n",
        "    data_subset = get_relevant_set(query, docs)\n",
        "    doc_id, data_corpus = extract_unstructured_data_for_relevance_classification(data_subset)\n",
        "    bm25 = get_bm25_model(data_corpus)\n",
        "    relevant_docs, irrelevant_docs = find_relevancy(query['unstructured_data'], bm25, doc_id)\n",
        "    ranked_list_after_rocchio = query_reformulator(query, data_subset, beta, gamma, rocchio_iteration, relevant_docs, irrelevant_docs)\n",
        "    ranked_list[query['qid']] = ranked_list_after_rocchio\n",
        "\n",
        "  json_object = json.dumps(ranked_list, indent = 4) \n",
        "  with open(\"ranked_list\"+t+fold+\".json\", \"w\") as outfile: \n",
        "    outfile.write(json_object) \n",
        "  return ranked_list\n",
        "\n",
        "import json\n",
        "def getRankers(docs, queries, beta, gamma):\n",
        "  rankers = []\n",
        "  test_rank_list = []\n",
        "  for r_itr in range(0,6):\n",
        "    print(\"**************R_itr = \", r_itr,\"**********************\")\n",
        "    ranked_list = train_model(queries, docs, r_itr, beta, gamma,\"1\",\"train\")\n",
        "    train_LambdaRank(ranked_list, get_corpus())\n",
        "  #   rankers.append(ranker)\n",
        "  # return rankers, test_rank_list\n",
        "\n",
        "def main():\n",
        "  docs =  extract_structured_data_for_exact_match()\n",
        "  queries = parse_query_xml_file()\n",
        "  score = []\n",
        "  beta = 0.75\n",
        "  gamma =  0.25\n",
        "  getRankers(docs, queries, beta, gamma)\n",
        "main()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************R_itr =  0 **********************\n",
            "Epoch 1/5\n",
            "16767/16767 [==============================] - 23s 1ms/step - loss: 0.0082\n",
            "Epoch 2/5\n",
            "16767/16767 [==============================] - 23s 1ms/step - loss: 0.0052\n",
            "Epoch 3/5\n",
            "16767/16767 [==============================] - 23s 1ms/step - loss: 0.0040\n",
            "Epoch 4/5\n",
            "16767/16767 [==============================] - 23s 1ms/step - loss: 0.0032\n",
            "Epoch 5/5\n",
            "16767/16767 [==============================] - 23s 1ms/step - loss: 0.0026\n",
            "ndcg: 0.6070941890124899\n",
            "ndcg@2: 0.18175295903539446\n",
            "**************R_itr =  1 **********************\n",
            "Epoch 1/5\n",
            "17157/17157 [==============================] - 23s 1ms/step - loss: 0.0074\n",
            "Epoch 2/5\n",
            "17157/17157 [==============================] - 24s 1ms/step - loss: 0.0031\n",
            "Epoch 3/5\n",
            "17157/17157 [==============================] - 24s 1ms/step - loss: 0.0021\n",
            "Epoch 4/5\n",
            "17157/17157 [==============================] - 24s 1ms/step - loss: 0.0017\n",
            "Epoch 5/5\n",
            "17157/17157 [==============================] - 26s 2ms/step - loss: 0.0015\n",
            "ndcg: 0.5869706002012687\n",
            "ndcg@2: 0.40657359638272916\n",
            "**************R_itr =  2 **********************\n",
            "Epoch 1/5\n",
            "17638/17638 [==============================] - 27s 2ms/step - loss: 0.0075\n",
            "Epoch 2/5\n",
            "17638/17638 [==============================] - 24s 1ms/step - loss: 0.0039\n",
            "Epoch 3/5\n",
            "17638/17638 [==============================] - 25s 1ms/step - loss: 0.0027\n",
            "Epoch 4/5\n",
            "17638/17638 [==============================] - 25s 1ms/step - loss: 0.0021\n",
            "Epoch 5/5\n",
            "17638/17638 [==============================] - 25s 1ms/step - loss: 0.0017\n",
            "ndcg: 0.5843239250883944\n",
            "ndcg@2: 0.020438239758848613\n",
            "**************R_itr =  3 **********************\n",
            "Epoch 1/5\n",
            "17988/17988 [==============================] - 25s 1ms/step - loss: 0.0060\n",
            "Epoch 2/5\n",
            "17988/17988 [==============================] - 25s 1ms/step - loss: 0.0019\n",
            "Epoch 3/5\n",
            "17988/17988 [==============================] - 24s 1ms/step - loss: 0.0013\n",
            "Epoch 4/5\n",
            "17988/17988 [==============================] - 24s 1ms/step - loss: 0.0011\n",
            "Epoch 5/5\n",
            "17988/17988 [==============================] - 25s 1ms/step - loss: 0.0010\n",
            "ndcg: 0.6637481851562791\n",
            "ndcg@2: 0.3528020232905472\n",
            "**************R_itr =  4 **********************\n",
            "Epoch 1/5\n",
            "18307/18307 [==============================] - 25s 1ms/step - loss: 0.0047\n",
            "Epoch 2/5\n",
            "18307/18307 [==============================] - 25s 1ms/step - loss: 0.0014\n",
            "Epoch 3/5\n",
            "18307/18307 [==============================] - 25s 1ms/step - loss: 0.0010\n",
            "Epoch 4/5\n",
            "18307/18307 [==============================] - 25s 1ms/step - loss: 8.6722e-04\n",
            "Epoch 5/5\n",
            "18307/18307 [==============================] - 25s 1ms/step - loss: 7.8836e-04\n",
            "ndcg: 0.6241069173475113\n",
            "ndcg@2: 0.2\n",
            "**************R_itr =  5 **********************\n",
            "Epoch 1/5\n",
            "18441/18441 [==============================] - 26s 1ms/step - loss: 0.0042\n",
            "Epoch 2/5\n",
            "18441/18441 [==============================] - 25s 1ms/step - loss: 0.0011\n",
            "Epoch 3/5\n",
            "18441/18441 [==============================] - 26s 1ms/step - loss: 8.0171e-04\n",
            "Epoch 4/5\n",
            "18441/18441 [==============================] - 27s 1ms/step - loss: 7.2002e-04\n",
            "Epoch 5/5\n",
            "18441/18441 [==============================] - 25s 1ms/step - loss: 6.5195e-04\n",
            "ndcg: 0.7687060563693974\n",
            "ndcg@2: 0.5355245321275766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0KGe88ndJWN"
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": []
    }
  ]
}